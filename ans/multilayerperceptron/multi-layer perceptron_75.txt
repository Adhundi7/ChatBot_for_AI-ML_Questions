In first order derivative of gradient descent, approximate the function as a line. In second order derivatove, approximate the function as a quadratic equation.(Better approximation) faster to true minima. There is a matrix coming in called hessian which is bid and sometimes need to be inverted. Due to this, though second order derivatives are faster in convergence, they are memory and compute intensive. so we go with first order which is GD.