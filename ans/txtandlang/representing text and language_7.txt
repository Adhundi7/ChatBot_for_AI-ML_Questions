In the 90s model we decided that if we want to recognise a face, what is important is the location of eyes, nose etc. I hardcoded programs that can detect the corners of the eyes, and so forth. Then the set of distances, the measurements, that is my representation of my face. But, when we started learning we will have some high level complex model, but it has got a bunch of parameters and those parameters I learnt from the data. In the first one there were no parameters, everything was hardcoded. The second one had a high level model which took parameters which had to be learnt from the data. The only difference is those set of parameters are so much more and the constraints imposed by the model is low, you practically learn from the data. You can’t 100% learn from the data there is some gap model, even if you assume that two things that are similar belong to same category, that’s the model that we use.                                                            Cont..                                                                                                That’s true. Even the saturation is not a single step. There are a bunch of techniques called metric learning algorithms and so on, we will discuss these. 

