The Neural Networks are over parameterized, which means there is lot of redundant neurons. When the neuron misses, the other neuron will be able to adjusts and wake up for that. For example, think about a hardware, if one unit fails, the weight some of the other connection weights also adjusted. That is why we believed it recovers from such failures. Whether it is a 10 in the hidden layer or 12 in the hidden layer there is no serious issue to predict.While training we always start with overparameterized model, then during deployment we reduce the number of neurons from 10 to 7 for example. These extra nodes ensures that no failures occur.