I'm only illustrating a very high level thing. There is also how the weights are changed. It also depends on the accuracy and stuff like that. So, I am not going to go into the full math of it. You need detailed probability analysis after that lets not get into that level. I think the idea is that you want to understand. You will see this class of methods where there are large numbers of weak classifiers and there is some way of combining them. The math of it, I don't think it is worth for us to get into at this time, we will if you are interested, we can discuss again, give you some reading material to to look into it. This is slightly old machine learning, in 1990s atleast, this was well known and people were trying to use this kind of method. But those days, the leading machine learning technique was support vector machines and that was kind of overpowering many of the other methods and people said bagging and boosting is nice, it will push up a little. But I can train using svm. It is anyway working. Why do I learn anything else. Then later on people tried multiple other things and one of the things somebody tried out was to try combining this idea of bagging with decision trees and that particular combination actually started to work extremely well and it was one of the best. It was equal to or better than support vector machines in many cases in those days. So these are all methods that has come out from the classical machine learning like support vector machines and random forest classifier which we will talk about which is basically combining trees using this kind of method and now deep learning and neural networks. These are all methods which are extremely strong.